{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: KISS (Keep it Short and Sweet): Pipeline\n",
    "Date: 2019-04-25 21:39\n",
    "Tags: pipeline\n",
    "Slug: pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSEMN Pipeline\n",
    "O — Obtaining our data\n",
    "S — Scrubbing / Cleaning our data\n",
    "E — Exploring / Visualizing our data will allow us to find patterns and trends\n",
    "M — Modeling our data will give us our predictive power as a wizard\n",
    "N — Interpreting our data\n",
    "\n",
    "O / L = obtaining data\n",
    "T / C = transforming cleaning\n",
    "E / V - exploring / visualizing\n",
    "M / P  - modeling, predicting\n",
    "N - intepreting\n",
    "\n",
    "\n",
    "List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer in scikit-learn - some class that have fit and transform method, or fit_transform method.\n",
    "\n",
    "Predictor - some class that has fit and predict methods, or fit_predict method.\n",
    "\n",
    "Pipeline is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator.\n",
    "\n",
    "Here is a good example of Pipeline usage. Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and now you can do something like:\n",
    "\n",
    "    vect = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = SGDClassifier()\n",
    "\n",
    "    vX = vect.fit_transform(Xtrain)\n",
    "    tfidfX = tfidf.fit_transform(vX)\n",
    "    predicted = clf.fit_predict(tfidfX)\n",
    "\n",
    "    # Now evaluate all steps on test set\n",
    "    vX = vect.fit_transform(Xtest)\n",
    "    tfidfX = tfidf.fit_transform(vX)\n",
    "    predicted = clf.fit_predict(tfidfX)\n",
    "With just:\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "predicted = pipeline.fit(Xtrain).predict(Xtrain)\n",
    "# Now evaluate all steps on test set\n",
    "predicted = pipeline.predict(Xtest)\n",
    "With pipelines you can easily perform a grid-search over set of parameters for each step of this meta-estimator. As described in the link above. All steps except last one must be transforms, last step can be transformer or predictor. Answer to edit: When you call pipln.fit() - each transformer inside pipeline will be fitted on outputs of previous transformer (First transformer is learned on raw dataset). Last estimator may be transformer or predictor, you can call fit_transform() on pipeline only if your last estimator is transformer (that implements fit_transform, or transform and fit methods separately), you can call fit_predict() or predict() on pipeline only if your last estimator is predictor. So you just can't call fit_transform or transform on pipeline, last step of which is predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
